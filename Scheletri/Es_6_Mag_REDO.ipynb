{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77e97ec2-857d-44c0-8b7f-c9b2c436362e",
   "metadata": {},
   "source": [
    "## Esame di Metodi Numerici 6 Maggio 2024 \n",
    "\n",
    "## Esercizo 1\n",
    "- Si consideri il sistema lineare Ax=b, con A matrice e b termine noto memorizzati nel file ``'test_14_09_2023.mat'``.  Risolvere il sistema confrontando almeno due tra i metodi  visti  per utilizzare per risolvere il sistema lineare con tale matrice dei coefficienti. Confrontare i risultati dei vari metodi, e giustificare i loro comportamento utilizzando i risultati teorici visti a lezione.\n",
    "- \n",
    "Per la lettura dei dati procedere nel seguente modo:\n",
    "\n",
    "``from scipy.io import loadmat``\n",
    "\n",
    "``import numpy as np``\n",
    "\n",
    "``dati = loadmat('test_06_05_2024.mat')``\n",
    "\n",
    "``A=dati[\"A\"] ``\n",
    "\n",
    "``A=A.astype(float)``\n",
    "\n",
    "`` b=dati[\"b\"] ``\n",
    "\n",
    "`` b=b.astype(float)``\n",
    "\n",
    "\n",
    "                                       [10 punti]\n",
    "                                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0d44a-0d01-44c1-b47f-9a155608f40b",
   "metadata": {},
   "source": [
    "- Data la matrice \n",
    "$$A=\\left[\n",
    "\\begin{array}{cccc}\n",
    "1 & 2 & 3 & 4\\\\\n",
    "2 & -4 & 6 & 8\\\\\n",
    "-1 & -2 & -3 & -1\\\\\n",
    "5 & 7 & 0 & 1\n",
    "\\end{array}\n",
    "\\right ],$$\n",
    "Richiamare le ipotesi sotto cui esiste la fattorizzazione di Gauss senza pivoting e scrivere un codice per  verificarle.\n",
    "\n",
    "                                                [2 punti]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbda924-2d3b-4357-bf34-cb936d3a4a87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Esercizio 2\n",
    "Scrivere uno script che calcoli il polinomio interpolante un insieme di punti $P_i =(x_i, y_i)$ $i = 0, ..., n $ nella forma di Lagrange, $n=5,10,15,18$\n",
    "\n",
    "- nodi $x_i$, punti equidistanti in un intervallo $[a, b]$,\n",
    "- nodi $x_i$, zeri dei polinomi di Chebyshev nell'intervallo $[a, b]$, ossia\n",
    "$$\n",
    "x_i = \\frac{(a + b)}{2}+\\frac{(b-a)}{2} \\, \\cos \\left(\n",
    "\\frac{(2i+1)\\pi}{2(n + 1)}\n",
    "\\right), \\quad  i =0, ..., n \n",
    "$$\n",
    " \n",
    "  e $y_i = f(x_i)$ ottenuti dalla valutazione nei punti $x_i$ della funzione test   $f: \\ [a, b] \\rightarrow {\\mathbb R}$. \n",
    "  - $f(x) = 1/(1+25*x^2)$,  $ \\quad x \\in [-1, 1]$ (funzione di Runge).\n",
    "  \n",
    "                                          [6] punti\n",
    "\n",
    "- Calcolare l'errore di interpolazione $r(x) =  f(x)-pe(x) $,\n",
    "tra la funzione test $f(x)$ e il polinomio di interpolazione $pe(x)$ calcolato a partire da nodi equdisitanti.\n",
    "                                        [1] punto\n",
    "                                        \n",
    "Visualizzare il grafico di $f(x)$ e $pe(x)$, ed il grafico di $|r(x)|$ per ogni valore $n=5,10,15,18$ \n",
    "\n",
    "                                        [1] punto\n",
    "                                        \n",
    "Calcolare l'errore di interpolazione $r(x) =  f(x)-pc(x) $,\n",
    "tra la funzione test $f(x)$ e il polinomio di interpolazione $p(x)$ calcolato a partire da nodi di Chebichev.\n",
    "\n",
    "                                      [1] punto\n",
    "                                            \n",
    "Visualizzare il grafico di $f(x)$ e $pc(x)$, ed il grafico di $|r(x)|$. \n",
    "\n",
    "                                       [1] punto\n",
    "\n",
    "Cosa si osserva? Cosa accade all'aumentare del grado $n$ di $p(x)$? Scrivere la formula dell'errore che si compie quando al posto della funzione che ha generato i dati si considera il polinomio interpolatore di grado n e commentarla.\n",
    "                                         \n",
    "                                         [3 punti]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4239d5c-a24b-432f-9903-e5d5e67f0669",
   "metadata": {},
   "source": [
    "**Domanda AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721a2f9-b321-4904-a015-98e1ff91f89c",
   "metadata": {},
   "source": [
    "- Descrivere gli elementi caratterizzanti di un MultiLayer Perceptron (MLP).( Com'è fatto un neurone artificiale, a caso servono le funzioni di attivazione, come sono organizzati i neuroni. Varie tipologie di reti MLP)  ed accennare in cosa consiste la fase di forward propagation e la fase di backward propagation. **Punti: 1**\n",
    "\n",
    "- Ottimizzazione della loss function per il training di una rete neurale per il task di regressione: Metodo di discesa del gradiente, metodo stocastico del gradiente, metodo del gradiente minibatch.  **Punti 1**  \n",
    " - Non convessità della loss-function - come non rimanere bloccati in un monimo locale? Metodo del gradiente con momentum. **Punti 2**\n",
    "- Learning rate scheduling: step decay, decadimento esponenziale, decadimento dipendente dal tempo. **Punti 1**\n",
    " - Learning rate adattivo: Adagrad, RMSProp, Adadelta, Adam. **Punti 2**\n",
    " \n",
    " **Totale:  7**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee72b0",
   "metadata": {},
   "source": [
    "### MultiLayer Perceptron (MLP)\n",
    "#### Struttura di un Neurone Artificiale\n",
    "Un neurone artificiale, l'elemento di base di una rete neurale, è modellato sulla base del neurone biologico. È costituito dai seguenti elementi:\n",
    "- **Pesi (Weights)**: Ogni input al neurone è moltiplicato per un peso che indica l'importanza relativa di quell'input.\n",
    "- **Sommatore (Summation)**: Gli input pesati sono sommati insieme.\n",
    "- **Bias**: Un termine di bias viene aggiunto alla somma per spostare la funzione di attivazione in modo da consentire alla rete di modellare i dati in modo più accurato.\n",
    "- **Funzione di Attivazione**: Una funzione non lineare applicata alla somma ponderata degli input. Le funzioni di attivazione comuni includono la funzione sigmoide, ReLU (Rectified Linear Unit), e tangente iperbolica.\n",
    "\n",
    "#### Funzioni di Attivazione\n",
    "Le funzioni di attivazione sono cruciali perché introducono non linearità nel modello, permettendo alla rete neurale di apprendere modelli complessi. Senza una funzione di attivazione non lineare, l'MLP si comporterebbe come un semplice modello lineare, anche con più strati.\n",
    "\n",
    "#### Organizzazione dei Neuroni\n",
    "In un MLP, i neuroni sono organizzati in strati:\n",
    "- **Strato di Input**: Il primo strato della rete, che riceve i dati grezzi.\n",
    "- **Strati Nascosti**: Questi strati elaborano gli input; possono esserci uno o più strati nascosti, ciascuno con un certo numero di neuroni.\n",
    "- **Strato di Output**: Lo strato finale, che fornisce l'output della rete.\n",
    "\n",
    "#### Tipologie di Reti MLP\n",
    "- **MLP Standard**: Un MLP classico è un feedforward neural network, in cui i dati passano dallo strato di input allo strato di output senza cicli.\n",
    "- **Deep Neural Networks (DNNs)**: Quando un MLP ha molti strati nascosti, viene chiamato \"deep,\" ed è capace di apprendere rappresentazioni più complesse.\n",
    "\n",
    "### Forward Propagation e Backward Propagation\n",
    "- **Forward Propagation**: Durante la forward propagation, gli input vengono passati attraverso i vari strati della rete. In ogni strato, vengono applicati i pesi, i bias, e la funzione di attivazione. Il risultato finale è l'output della rete, che viene confrontato con il target per calcolare la loss.\n",
    "  \n",
    "- **Backward Propagation**: La backpropagation è il processo di aggiornamento dei pesi della rete in base all'errore commesso. Viene calcolato il gradiente della funzione di perdita rispetto ai pesi, e i pesi vengono aggiornati utilizzando il metodo della discesa del gradiente.\n",
    "\n",
    "### Ottimizzazione della Loss Function\n",
    "- **Metodo della Discesa del Gradiente (Gradient Descent)**: Un metodo iterativo per minimizzare la funzione di perdita, aggiornando i pesi nella direzione opposta al gradiente della funzione di perdita.\n",
    "  \n",
    "- **Metodo Stocastico del Gradiente (SGD)**: Una variante della discesa del gradiente dove ogni aggiornamento dei pesi è fatto utilizzando un singolo campione, il che può accelerare il processo ma introduce rumore nelle stime del gradiente.\n",
    "  \n",
    "- **Metodo del Gradiente Minibatch**: Combina i vantaggi del batch gradient descent e dell'SGD, aggiornando i pesi basati su un piccolo lotto di campioni invece che su un singolo campione o sull'intero dataset.\n",
    "\n",
    "### Non Convessità della Loss-Function e Momentum\n",
    "La funzione di perdita in una rete neurale è spesso non convessa, il che significa che può avere molti minimi locali. Per evitare di rimanere bloccati in un minimo locale:\n",
    "- **Metodo del Gradiente con Momentum**: Introduce un termine che accelera l'aggiornamento dei pesi lungo le direzioni in cui il gradiente è costante, riducendo le oscillazioni e aiutando a superare i minimi locali.\n",
    "\n",
    "### Learning Rate Scheduling\n",
    "Il learning rate è un parametro che controlla l'entità degli aggiornamenti dei pesi. Un learning rate troppo alto può far oscillare l'algoritmo, mentre uno troppo basso rallenta la convergenza.\n",
    "- **Step Decay**: Il learning rate viene ridotto di un fattore fisso ogni volta che il numero di epoche raggiunge un certo valore.\n",
    "- **Decadimento Esponenziale**: Il learning rate viene ridotto in modo esponenziale nel corso del tempo.\n",
    "- **Decadimento Dipendente dal Tempo**: Il learning rate diminuisce proporzionalmente all'inverso del tempo.\n",
    "\n",
    "### Learning Rate Adattivo\n",
    "Metodi avanzati per adattare il learning rate automaticamente durante l'addestramento:\n",
    "- **Adagrad**: Adatta il learning rate per ogni parametro, basandosi sulla somma dei quadrati dei gradienti precedenti.\n",
    "- **RMSProp**: Modifica Adagrad per mantenere la somma dei gradienti in una finestra mobile, evitando che il learning rate diventi troppo piccolo.\n",
    "- **Adadelta**: Ulteriore miglioramento di RMSProp, elimina la necessità di specificare manualmente un learning rate iniziale.\n",
    "- **Adam**: Combina i vantaggi di RMSProp e del metodo del momentum, mantenendo una media mobile sia del gradiente che del quadrato del gradiente. Adam è uno dei metodi più popolari per l'ottimizzazione delle reti neurali."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
