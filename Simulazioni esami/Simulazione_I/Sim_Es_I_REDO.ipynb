{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edf6b43-5c77-43f5-a179-9986fc68abf8",
   "metadata": {},
   "source": [
    "## Simulazione dell'esame di Metodi Numerici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8dd546-5e8d-4dde-ab61-51251c05bad2",
   "metadata": {},
   "source": [
    "## Esercizio 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378dc3b4-702c-4e2f-b388-fc5275db9a75",
   "metadata": {},
   "source": [
    "# Esercizio 1\n",
    "Nel file ``testI.mat`` sono memorizzati la matrice A ed il vettore b. \n",
    "\n",
    " - Verificare che per risolvere il sistema Ax=b sia possibile utilizzare i due metodi di discesa visti a lezione. Richiamare il teorema che  garantisce che risolvere un sistema lineare con specifiche proprietà equivale a trovare il minimo di un opportuni funzionale quadratico.  [**Punti 2**]\n",
    " - implementare entrambi gli algoritmi e dire quante iterazioni sono necessarie per ciascuno di essi  per calcolare la soluzione con una toll=1e-6 e maxit=4000. Visualizzare in un grafico l'errore in scala logaritmica ad ogni iterazione per ciascuno dei due metodi. [**Punti 5**]\n",
    " - Verificare se la matrice è  malcondizionata, dire teoricamente cosa questo implica in termini di velocità di  convergenza alla soluzione di ciascuno dei due metodi, richiamando il risultato teorico visto a lezione. [**Punti 2**]\n",
    "\n",
    " - Costruire la matrice A1, ottenuta a partire da A sommandole una matrice diagonale con elementi sulla\n",
    "      diagonali tutti uguali a 0.05\n",
    "   nel seguente modo:\n",
    "   \n",
    "   A1=A+np.eye(A.shape[0])*0.05\n",
    "   \n",
    "   e termine noto b1, costruito in maniera tale che la soluzione del sistema A1 x1 =b1 sia il vettore formato da tutti 1.  Risolvere il sistema lineare con matrice dei coefficienti A1 e termine noto b1, sia con il metodo del gradiente che con il metodo del gradiente coniugato. Osservare il numero di iterazioni eseguito da ciascun metodo e giustificare i risultati  [**Punti 2**]\n",
    "  \n",
    "   **Totale: Punti 11**\n",
    "\n",
    "Per la lettura dei dati procedere nel seguente modo:\n",
    "\n",
    "``from scipy.io import loadmat``\n",
    "\n",
    "``import numpy as np``\n",
    "\n",
    "``dati = loadmat('testI.mat')``\n",
    "\n",
    "``A=dati[\"A\"] ``\n",
    "\n",
    "``A=A.astype(float)``\n",
    "\n",
    "`` b=dati[\"b\"] ``\n",
    "\n",
    "`` b=b.astype(float)``\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29d3fc-3800-4a2e-93a1-9c31f878c266",
   "metadata": {},
   "source": [
    "## Esercizio 2 ##\n",
    "\n",
    "Siano assegnati i numpy array\n",
    "\n",
    "x =[1.0,1.2,1.4,1.6,1.8,2.0,2.2,2.4,2.6,2.8,3.0] \n",
    "\n",
    "y =[1.18,1.26,1.23,1.37,1.37,1.45,1.42,1.46,1.53,1.59,1.59] \n",
    "                                                               \n",
    "contenenti rispettivamente le ascisse e le ordinate di 11 punti nel piano.                                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa94727-34cb-4782-b764-c56b7e95f321",
   "metadata": {},
   "source": [
    "Scrivere lo script python in cui:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd6520-ed15-4c05-bf8c-1261c002b60f",
   "metadata": {},
   "source": [
    "- si calcola il polinomio di approssimazione ai minimi quadrati di grado 1 dei punti assegnati, sviluppando le functions necessarie. e lo si valuti su 200 punti equidistanti nell'intervallo [1,3]  [**Punti: 4**]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b9058-2505-4f5e-97c8-6db9a372d025",
   "metadata": {},
   "source": [
    "- si calcola il polinomio di interpolazione dei punti assegnati sviluppando le functions necessarie e lo si valuti su 200 punti equidistanti nell'intervallo [1,3]  [**Punti: 4**]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf2e51-4085-4cfb-a411-e04d142ff647",
   "metadata": {},
   "source": [
    "- Si modifichino le ordinate dei punti assegnati, facendo uso della formula   $\\hat{y_i}=2*x_i+1$, $i=1,...,11$, e si ricalcoli il polinomio interpolante ed il polinomio di approssimazione di grado 1, a partire dalle coppie $(x_i,\\hat{y_i})$, $i=1,11$   [**Punti: 1**]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758a777-c7c6-4459-bdc7-8e6ec235ca98",
   "metadata": {},
   "source": [
    "- Si visualizzi in un grafico il polinomio interpolante, il polinomio di approssimazione di grado 1 ed i punti $(x_i,y_i), i=1,11$ ed in un altro grafico il polinomio interpolante, il polinomio di approssimazione di grado 1 ed i $(x_i,\\hat{y_i})$, $i=1,11$  [**Punti:1**]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ba739-c5b7-4f04-8f37-30f1584c2e90",
   "metadata": {},
   "source": [
    "- Si fornisca una spiegazione teorica dei risultati ottenuti\n",
    "  [**Punti: 4**]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c8888-3dfe-4ea5-a93b-e0adab565e1b",
   "metadata": {},
   "source": [
    "                                                                        Totale 14 punti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911860d3-45d8-45fc-99f2-bf79c64f1a80",
   "metadata": {},
   "source": [
    "## Domanda intelligenza artificiale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f72b3e-3533-4866-bf02-44fea1b7c825",
   "metadata": {},
   "source": [
    "- Importanza del learning rate nell'allenamento di una rete neurale \n",
    "[**Punti: 1**]\n",
    "- Illustra l'algoritmo di backpropagation per il calcolo delle derivate parziale della funzione costo rispetto ai pesi di tutti i layer, nel caso di una MLP con 1 nodo di input, 3 layer nascosti formati da un solo nodo, ed 1 nodo di output. [**Punti: 4**] \n",
    " \n",
    "- Tecniche di Ottimizzazione: metodo di discesa del gradient batch, metodo del gradiente stocastico (SGD) ,metodo del gradiente stocastico minibatch. [**Punti: 2**]\n",
    "\n",
    "\n",
    "    [**Totale: punti 7**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f7239",
   "metadata": {},
   "source": [
    "### 1. **Importanza del Learning Rate nell'Allenamento di una Rete Neurale** [**Punti: 1**]\n",
    "\n",
    "Il **learning rate** è un iperparametro critico nell'allenamento di una rete neurale, che determina la dimensione del passo con cui l'algoritmo di ottimizzazione aggiorna i pesi del modello. \n",
    "\n",
    "- **Learning rate troppo basso:** Con un valore troppo basso, il modello potrebbe impiegare troppo tempo per convergere, e in alcuni casi, potrebbe rimanere bloccato in un minimo locale, rendendo l'allenamento inefficiente.\n",
    "\n",
    "- **Learning rate troppo alto:** Un valore troppo alto può causare salti troppo grandi durante l'aggiornamento dei pesi, che potrebbe portare il modello a divergere o oscillare intorno a un punto senza mai convergere.\n",
    "\n",
    "In sintesi, scegliere il corretto learning rate è essenziale per assicurare una rapida convergenza e la stabilità del processo di allenamento, consentendo al modello di raggiungere un buon minimo della funzione di costo.\n",
    "\n",
    "### 2. **Algoritmo di Backpropagation per il Calcolo delle Derivate Parziali della Funzione Costo rispetto ai Pesi di Tutti i Layer, nel Caso di una MLP con 1 Nodo di Input, 3 Layer Nascosti Formati da un Solo Nodo, e 1 Nodo di Output** [**Punti: 4**]\n",
    "\n",
    "Consideriamo una rete neurale MLP (Multilayer Perceptron) con la seguente architettura:\n",
    "- 1 nodo di input ($x$)\n",
    "- 3 layer nascosti, ciascuno formato da un solo nodo ($h_1$, $h_2$, $h_3$)\n",
    "- 1 nodo di output ($o$)\n",
    "\n",
    "Ogni nodo $h_i$ ha un peso associato $w_i$, e l'output finale ha un peso $w_o$.\n",
    "\n",
    "#### Forward Propagation:\n",
    "\n",
    "1. **Calcolo dei valori nei nodi nascosti:**\n",
    "   $$\n",
    "   h_1 = f(w_1 \\cdot x)\n",
    "   $$\n",
    "   $$\n",
    "   h_2 = f(w_2 \\cdot h_1)\n",
    "   $$\n",
    "   $$\n",
    "   h_3 = f(w_3 \\cdot h_2)\n",
    "   $$\n",
    "   \n",
    "   Dove $f(\\cdot)$ è la funzione di attivazione, ad esempio ReLU o Sigmoide.\n",
    "\n",
    "2. **Calcolo dell'output:**\n",
    "   $$\n",
    "   o = f(w_o \\cdot h_3)\n",
    "   $$\n",
    "   \n",
    "   La funzione di costo per un singolo esempio è data da:\n",
    "   $$\n",
    "   C = \\frac{1}{2} \\cdot (o - y)^2\n",
    "   $$\n",
    "   Dove $y$ è l'etichetta corretta.\n",
    "\n",
    "#### Backward Propagation:\n",
    "\n",
    "1. **Calcolo del gradiente dell'errore rispetto all'output:**\n",
    "   $$\n",
    "   \\frac{\\partial C}{\\partial o} = o - y\n",
    "   $$\n",
    "\n",
    "2. **Calcolo del gradiente rispetto a $w_o$:**\n",
    "   $$\n",
    "   \\frac{\\partial C}{\\partial w_o} = \\frac{\\partial C}{\\partial o} \\cdot \\frac{\\partial o}{\\partial w_o} = (o - y) \\cdot \\frac{\\partial f(w_o \\cdot h_3)}{\\partial w_o}\n",
    "   $$\n",
    "\n",
    "3. **Backpropagation nel terzo layer nascosto ($h_3$):**\n",
    "   $$\n",
    "   \\frac{\\partial C}{\\partial h_3} = \\frac{\\partial C}{\\partial o} \\cdot w_o\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial C}{\\partial w_3} = \\frac{\\partial C}{\\partial h_3} \\cdot \\frac{\\partial h_3}{\\partial w_3} = \\left(\\frac{\\partial C}{\\partial o} \\cdot w_o\\right) \\cdot \\frac{\\partial f(w_3 \\cdot h_2)}{\\partial w_3}\n",
    "   $$\n",
    "\n",
    "4. **Backpropagation nel secondo layer nascosto ($h_2$):**\n",
    "   $$\n",
    "   \\frac{\\partial C}{\\partial h_2} = \\frac{\\partial C}{\\partial h_3} \\cdot w_3\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial C}{\\partial w_2} = \\frac{\\partial C}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial w_2} = \\left(\\frac{\\partial C}{\\partial h_3} \\cdot w_3\\right) \\cdot \\frac{\\partial f(w_2 \\cdot h_1)}{\\partial w_2}\n",
    "   $$\n",
    "\n",
    "5. **Backpropagation nel primo layer nascosto ($h_1$):**\n",
    "   $$\n",
    "   \\frac{\\partial C}{\\partial h_1} = \\frac{\\partial C}{\\partial h_2} \\cdot w_2\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial C}{\\partial w_1} = \\frac{\\partial C}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_1} = \\left(\\frac{\\partial C}{\\partial h_2} \\cdot w_2\\right) \\cdot \\frac{\\partial f(w_1 \\cdot x)}{\\partial w_1}\n",
    "   $$\n",
    "\n",
    "#### Aggiornamento dei Pesi:\n",
    "I pesi vengono aggiornati utilizzando la regola del gradiente discendente:\n",
    "$$\n",
    "w_i \\leftarrow w_i - \\eta \\cdot \\frac{\\partial C}{\\partial w_i}\n",
    "$$\n",
    "Dove $\\eta$ è il learning rate.\n",
    "\n",
    "### 3. **Tecniche di Ottimizzazione: Metodo di Discesa del Gradient Batch, Metodo del Gradiente Stocastico (SGD), Metodo del Gradiente Stocastico Minibatch** [**Punti: 2**]\n",
    "\n",
    "#### a) **Metodo di Discesa del Gradiente Batch:**\n",
    "- **Descrizione:** In questo metodo, il gradiente della funzione costo viene calcolato utilizzando l'intero dataset. I pesi vengono aggiornati dopo aver processato tutti i campioni di dati.\n",
    "- **Vantaggi:** Produce aggiornamenti dei pesi più stabili e accurati.\n",
    "- **Svantaggi:** Lento su dataset di grandi dimensioni, richiede molta memoria.\n",
    "\n",
    "#### b) **Metodo del Gradiente Stocastico (SGD):**\n",
    "- **Descrizione:** In questo metodo, i pesi vengono aggiornati dopo ogni singolo campione di dati. Questo rende gli aggiornamenti più frequenti e spesso più rapidi.\n",
    "- **Vantaggi:** Più veloce per grandi dataset, richiede meno memoria.\n",
    "- **Svantaggi:** Gli aggiornamenti dei pesi sono meno stabili, possono portare a oscillazioni nella funzione di costo.\n",
    "\n",
    "#### c) **Metodo del Gradiente Stocastico Minibatch:**\n",
    "- **Descrizione:** Combina i vantaggi del batch e dello SGD. I pesi vengono aggiornati dopo aver processato un piccolo batch di campioni di dati.\n",
    "- **Vantaggi:** Compromesso tra stabilità e velocità, più efficiente per l'implementazione in parallelo.\n",
    "- **Svantaggi:** La scelta della dimensione del batch è cruciale per il bilanciamento tra la convergenza stabile e la velocità di apprendimento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
